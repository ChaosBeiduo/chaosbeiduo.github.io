<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="theme-color" content="#222"><meta name="generator" content="Hexo 7.3.0">

  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">



<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.5.2/css/all.min.css" integrity="sha256-XOqroi11tY4EFQMR9ZYwZWKj5ZXiftSx36RRuC3anlA=" crossorigin="anonymous">

<script class="next-config" data-name="main" type="application/json">{"hostname":"example.com","root":"/","images":"/images","scheme":"Muse","darkmode":false,"version":"8.20.0","exturl":false,"sidebar":{"position":"left","width_expanded":320,"width_dual_column":240,"display":"post","padding":18,"offset":12},"copycode":{"enable":false,"style":null},"fold":{"enable":false,"height":500},"bookmark":{"enable":false,"color":"#222","save":"auto"},"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"stickytabs":false,"motion":{"enable":false,"async":false,"transition":{"menu_item":"fadeInDown","post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"i18n":{"placeholder":"Searching...","empty":"We didn't find any results for the search: ${query}","hits_time":"${hits} results found in ${time} ms","hits":"${hits} results found"}}</script><script src="/js/config.js"></script>

    <meta name="description" content="OverviewGemma is a family of lightweight, state-of-the art open models built from the same research and technology used to create the Gemini models. In this notebook, we demonstrate how to use KerasNL">
<meta property="og:type" content="article">
<meta property="og:title" content="[Gemma-2] Fine-Tune Gemma 2 in Keras Using LoRA">
<meta property="og:url" content="http://example.com/2024/12/02/Gemma-2-Tuning/index.html">
<meta property="og:site_name" content="CHAOSLAND">
<meta property="og:description" content="OverviewGemma is a family of lightweight, state-of-the art open models built from the same research and technology used to create the Gemini models. In this notebook, we demonstrate how to use KerasNL">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="https://raw.githubusercontent.com/ChaosBeiduo/StoreHouse/main/lora_output_before.png">
<meta property="og:image" content="https://raw.githubusercontent.com/ChaosBeiduo/StoreHouse/main/lora_output_after.png">
<meta property="article:published_time" content="2024-12-02T10:36:51.000Z">
<meta property="article:modified_time" content="2024-12-06T03:56:04.148Z">
<meta property="article:author" content="カオスベイド">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://raw.githubusercontent.com/ChaosBeiduo/StoreHouse/main/lora_output_before.png">


<link rel="canonical" href="http://example.com/2024/12/02/Gemma-2-Tuning/">



<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":false,"isPost":true,"lang":"en","comments":true,"permalink":"http://example.com/2024/12/02/Gemma-2-Tuning/","path":"2024/12/02/Gemma-2-Tuning/","title":"[Gemma-2] Fine-Tune Gemma 2 in Keras Using LoRA"}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>[Gemma-2] Fine-Tune Gemma 2 in Keras Using LoRA | CHAOSLAND</title>
  








  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="headband"></div>

  <main class="main">
    <div class="column">
      <header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar" role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <p class="site-title">CHAOSLAND</p>
      <i class="logo-line"></i>
    </a>
      <p class="site-subtitle" itemprop="description">My Odyssey in Programming</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger" aria-label="Search" role="button">
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu"><li class="menu-item menu-item-about"><a href="/about/" rel="section"><i class="fa-solid fa-user-astronaut fa-fw"></i>About</a></li><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa-solid fa-dungeon fa-fw"></i>Home</a></li><li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="fa-solid fa-toilet-paper fa-fw"></i>Categories</a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa-solid fa-box fa-fw"></i>Archives</a></li>
  </ul>
</nav>




</header>
        
  
  <aside class="sidebar">

    <div class="sidebar-inner sidebar-nav-active sidebar-toc-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
            <div class="post-toc animated"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#Overview"><span class="nav-number">1.</span> <span class="nav-text">Overview</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Preparation"><span class="nav-number">2.</span> <span class="nav-text">Preparation</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Get-access-to-Gemma"><span class="nav-number">2.1.</span> <span class="nav-text">Get access to Gemma</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Install-dependencies"><span class="nav-number">2.2.</span> <span class="nav-text">Install dependencies</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Select-a-backend"><span class="nav-number">2.3.</span> <span class="nav-text">Select a backend</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Import-packages"><span class="nav-number">2.4.</span> <span class="nav-text">Import packages</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Prepare-dataset"><span class="nav-number">3.</span> <span class="nav-text">Prepare dataset</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Create-a-model"><span class="nav-number">4.</span> <span class="nav-text">Create a model</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Inference-before-finetuning"><span class="nav-number">5.</span> <span class="nav-text">Inference before finetuning</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Finetuning"><span class="nav-number">6.</span> <span class="nav-text">Finetuning</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Inference-after-fine-tuning"><span class="nav-number">6.1.</span> <span class="nav-text">Inference after fine-tuning</span></a></li></ol></li></ol></div>
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="カオスベイド"
      src="/images/avatar.jpg">
  <p class="site-author-name" itemprop="name">カオスベイド</p>
  <div class="site-description" itemprop="description">Document my own journey like a director.</div>
</div>
<div class="site-state-wrap animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
        <a href="/archives/">
          <span class="site-state-item-count">7</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
          <a href="/categories/">
        <span class="site-state-item-count">8</span>
        <span class="site-state-item-name">categories</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author animated">
      <span class="links-of-author-item">
        <a href="https://github.com/ChaosBeiduo" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;ChaosBeiduo" rel="noopener me" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="https://www.linkedin.com/in/yiwenzhang233/" title="LinkedIn → https:&#x2F;&#x2F;www.linkedin.com&#x2F;in&#x2F;yiwenzhang233&#x2F;" rel="noopener me" target="_blank"><i class="fab fa-linkedin fa-fw"></i>LinkedIn</a>
      </span>
      <span class="links-of-author-item">
        <a href="https://www.kaggle.com/chaoschako" title="Kaggle → https:&#x2F;&#x2F;www.kaggle.com&#x2F;chaoschako" rel="noopener me" target="_blank"><i class="fa-brands fa-kaggle fa-fw"></i>Kaggle</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:chaosbeiduo@outlook.com" title="chaosbeiduo@outlook.com → mailto:chaosbeiduo@outlook.com" rel="noopener me" target="_blank"><i class="fa fa-envelope fa-fw"></i>chaosbeiduo@outlook.com</a>
      </span>
  </div>

        </div>
      </div>
    </div>

    
    <div class="sidebar-inner sidebar-blogroll">
      <div class="links-of-blogroll animated">
        <div class="links-of-blogroll-title"><i class="fa fa-globe fa-fw"></i>
          Links
        </div>
        <ul class="links-of-blogroll-list">
            <li class="links-of-blogroll-item">
              <a href="https://spinning-wheel-nine.vercel.app/" title="https:&#x2F;&#x2F;spinning-wheel-nine.vercel.app&#x2F;" rel="noopener" target="_blank">Spinning-Wheel</a>
            </li>
        </ul>
      </div>
    </div>
  </aside>


    </div>

    <div class="main-inner post posts-expand">


  


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/2024/12/02/Gemma-2-Tuning/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.jpg">
      <meta itemprop="name" content="カオスベイド">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="CHAOSLAND">
      <meta itemprop="description" content="Document my own journey like a director.">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="[Gemma-2] Fine-Tune Gemma 2 in Keras Using LoRA | CHAOSLAND">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          [Gemma-2] Fine-Tune Gemma 2 in Keras Using LoRA
        </h1>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2024-12-02 18:36:51" itemprop="dateCreated datePublished" datetime="2024-12-02T18:36:51+08:00">2024-12-02</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">Edited on</span>
      <time title="Modified: 2024-12-06 11:56:04" itemprop="dateModified" datetime="2024-12-06T11:56:04+08:00">2024-12-06</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">In</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/Large-Language-Models/" itemprop="url" rel="index"><span itemprop="name">Large Language Models</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody"><h2 id="Overview"><a href="#Overview" class="headerlink" title="Overview"></a>Overview</h2><p>Gemma is a family of lightweight, state-of-the art open models built from the same research and technology used to create the Gemini models.</p>
<p>In this notebook, we demonstrate how to use <strong>KerasNLP</strong> to perform <strong>LoRA fine-tuning</strong> on a <strong>Gemma 2B</strong> model. The fine-tuning process leverages the <a target="_blank" rel="noopener" href="https://huggingface.co/datasets/databricks/databricks-dolly-15k">Databricks Dolly 15k dataset</a>, a curated collection of 15,000 high-quality, human-generated prompt-response pairs. This dataset is specifically designed to facilitate fine-tuning for large language models (LLMs), making it an excellent choice for this task.</p>
<span id="more"></span>

<h2 id="Preparation"><a href="#Preparation" class="headerlink" title="Preparation"></a>Preparation</h2><h3 id="Get-access-to-Gemma"><a href="#Get-access-to-Gemma" class="headerlink" title="Get access to Gemma"></a>Get access to Gemma</h3><ol>
<li><p>Request for <a target="_blank" rel="noopener" href="https://www.kaggle.com/models/google/gemma-2">Gemma 2</a></p>
</li>
<li><p>Create a Workbook runtime with sufficient resources to run the Gemma 2B model.</p>
</li>
<li><p>Better select the T4 GPU Accelerator:</p>
<ul>
<li>In the upper-left of the Kaggle window, select <strong>Settings</strong> -&gt; ➡<strong>Accelerator</strong>.</li>
<li>select <strong>GPU T4 x2</strong> .</li>
</ul>
</li>
</ol>
<h3 id="Install-dependencies"><a href="#Install-dependencies" class="headerlink" title="Install dependencies"></a>Install dependencies</h3><p>Install Keras, KerasNLP, and other dependencies.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Install Keras 3 last. See https://keras.io/getting_started/ for more details.</span></span><br><span class="line">!pip install -q -U keras-nlp</span><br><span class="line">!pip install -q -U <span class="string">&quot;keras&gt;=3&quot;</span></span><br><span class="line"><span class="comment"># Perhaps you need to install this</span></span><br><span class="line">!pip install einops</span><br></pre></td></tr></table></figure>

<p><strong>Keras provides a powerful general-purpose deep learning framework, while <code>keras_nlp</code> focuses on NLP tasks.</strong></p>
<p>When working on natural language processing (NLP), choosing the right tools is key. Keras is a high-level API known for its simplicity and flexibility, supporting a wide range of tasks like image recognition and time-series analysis, making it ideal for general-purpose machine learning.</p>
<p>On the other hand, <code>keras_nlp</code> extends Keras specifically for NLP, offering optimized models, tokenization tools, and preprocessing utilities tailored for text-based tasks. By focusing on NLP, <code>keras_nlp</code> streamlines the process of handling text data and fine-tuning pre-trained models, boosting efficiency for NLP applications.</p>
<p>Together, these libraries complement each other, allowing developers to leverage Keras’ broad capabilities alongside the specialized features of <code>keras_nlp</code> for language processing.</p>
<h3 id="Select-a-backend"><a href="#Select-a-backend" class="headerlink" title="Select a backend"></a>Select a backend</h3><p>Keras is a high-level, multi-framework deep learning API designed for simplicity and ease of use. Using Keras 3, you can run workflows on one of three backends: TensorFlow, JAX, or PyTorch.</p>
<p>For this tutorial, configure the backend for JAX.</p>
<p><code>XLA_PYTHON_CLIENT_MEM_FRACTION</code>, controls the fraction of GPU memory allocated to the XLA (Accelerated Linear Algebra) backend when running with the JAX backend.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"></span><br><span class="line"><span class="comment"># Keras 3 lets you choose the backend: TensorFlow, JAX, or PyTorch. </span></span><br><span class="line">os.environ[<span class="string">&quot;KERAS_BACKEND&quot;</span>] = <span class="string">&quot;jax&quot;</span></span><br><span class="line"><span class="comment"># Avoid memory fragmentation on JAX backend.</span></span><br><span class="line">os.environ[<span class="string">&quot;XLA_PYTHON_CLIENT_MEM_FRACTION&quot;</span>]=<span class="string">&quot;1.00&quot;</span></span><br></pre></td></tr></table></figure>



<h3 id="Import-packages"><a href="#Import-packages" class="headerlink" title="Import packages"></a>Import packages</h3><p>Import Keras and KerasNLP.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> keras</span><br><span class="line"><span class="keyword">import</span> keras_nlp</span><br></pre></td></tr></table></figure>



<h2 id="Prepare-dataset"><a href="#Prepare-dataset" class="headerlink" title="Prepare dataset"></a>Prepare dataset</h2><p>While Hugging Face offers an easy way to download datasets, the dataset we’re using may not be in the ideal format for fine-tuning. Specifically, we need to reformat the data into a structure where each entry consists of a <em>prompt</em> and a <em>response</em>.</p>
<ol>
<li>Load dataset</li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> datasets <span class="keyword">import</span> load_dataset</span><br><span class="line"></span><br><span class="line">dataset = load_dataset(<span class="string">&quot;databricks/databricks-dolly-15k&quot;</span>, trust_remote_code=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(dataset)</span><br></pre></td></tr></table></figure>

<blockquote>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">DatasetDict(&#123;</span><br><span class="line"> 	train: Dataset(&#123;</span><br><span class="line">     	features: [&#x27;instruction&#x27;, &#x27;context&#x27;, &#x27;response&#x27;, &#x27;category&#x27;],</span><br><span class="line">     	num_rows: 15011</span><br><span class="line"> 	&#125;)</span><br><span class="line">&#125;)</span><br></pre></td></tr></table></figure>
</blockquote>
<ol start="2">
<li>Preprocess the data</li>
</ol>
<p>After downloading the dataset, we’ll preprocess it to match the required format.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">data = []</span><br><span class="line">template = <span class="string">&quot;Instruction:\n&#123;instruction&#125;\n\nResponse:\n&#123;response&#125;&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> item <span class="keyword">in</span> dataset[<span class="string">&#x27;train&#x27;</span>]:</span><br><span class="line">    data.append(template.<span class="built_in">format</span>(prompt=item[<span class="string">&quot;instruction&quot;</span>], response=item[<span class="string">&quot;response&quot;</span>]))</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(data[:<span class="number">5</span>])</span><br></pre></td></tr></table></figure>

<blockquote>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[&#x27;User:\nWhen did Virgin Australia start operating?\nModel:\nVirgin Australia commenced services on 31 August 2000 as Virgin Blue, with two aircraft on a single route.&#x27;, &#x27;User:\nWhich is a species of fish? Tope or Rope\nModel:\nTope&#x27;, &#x27;User:\nWhy can camels survive for long without water?\nModel:\nCamels use the fat in their humps to keep them filled with energy and hydration for long periods of time.&#x27;, &quot;User:\nAlice&#x27;s parents have three daughters: Amy, Jessy, and what’s the name of the third daughter?\nModel:\nThe name of the third daughter is Alice&quot;, &#x27;User:\nWhen was Tomoaki Komorida born?\nModel:\nTomoaki Komorida was born on July 10,1981.&#x27;]</span><br></pre></td></tr></table></figure>
</blockquote>
<p>Truncate our data to speed up training.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">data = data[:<span class="number">2500</span>]</span><br></pre></td></tr></table></figure>



<h2 id="Create-a-model"><a href="#Create-a-model" class="headerlink" title="Create a model"></a>Create a model</h2><p>KerasNLP provides implementations of many popular model architectures. In this notebook, we’ll create a model using <code>GemmaCausalLM</code>, an end-to-end Gemma model designed for causal language modeling. A causal language model predicts the next token based on previous tokens.</p>
<p>We’ll use the <code>from_preset</code> method to initialize the model.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">gemma_lm = keras_nlp.models.GemmaCausalLM.from_preset(<span class="string">&quot;gemma2_2b_en&quot;</span>)</span><br><span class="line">gemma_lm.summary()</span><br></pre></td></tr></table></figure>

<p><img src="https://raw.githubusercontent.com/ChaosBeiduo/StoreHouse/main/lora_output_before.png"></p>
<h2 id="Inference-before-finetuning"><a href="#Inference-before-finetuning" class="headerlink" title="Inference before finetuning"></a>Inference before finetuning</h2><p>Before we proceed with fine-tuning a language model like Gemma, it’s important to understand how the model generates text with an initial prompt. In this example, we demonstrate how to perform inference using the pre-trained Gemma model before applying any fine-tuning.</p>
<ol>
<li><p><strong>Define the Prompt and Response</strong>:<br>We start by setting a prompt to feed into the model. In this case, the prompt is <code>&quot;All work and no play makes Jack a dull boy&quot;</code>, a well-known phrase. The response is initialized as an empty string because we will let the model generate the continuation.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">prompt = template.<span class="built_in">format</span>(</span><br><span class="line">    prompt=<span class="string">&quot;All work and no play makes Jack a dull boy&quot;</span>,</span><br><span class="line">    response=<span class="string">&quot;&quot;</span>,</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
</li>
<li><p><strong>Sampling Strategy</strong>:<br>We use <code>keras_nlp.samplers.TopKSampler</code> with a parameter <code>k=5</code> to restrict the sampling to the top 5 most probable next words at each step. The <code>seed=2</code> ensures that the randomness of the sampling is reproducible, making the experiment consistent across runs.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sampler = keras_nlp.samplers.TopKSampler(k=<span class="number">5</span>, seed=<span class="number">2</span>)</span><br></pre></td></tr></table></figure>
</li>
<li><p><strong>Compile the Model</strong>:<br>Next, we compile the Gemma model by passing the sampler to guide the text generation process. The sampler controls how the model picks the next word in the sequence.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">gemma_lm.<span class="built_in">compile</span>(sampler=sampler)</span><br></pre></td></tr></table></figure>
</li>
<li><p><strong>Text Generation</strong>:<br>Finally, we use the <code>generate</code> method to generate a text sequence from the prompt. We specify <code>max_length=256</code>, meaning that the model will generate text up to 256 tokens long.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(gemma_lm.generate(prompt, max_length=<span class="number">256</span>))</span><br></pre></td></tr></table></figure>

<blockquote>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">Instruction:</span><br><span class="line">All work and no play makes Jack a dull boy</span><br><span class="line"></span><br><span class="line">Response:</span><br><span class="line">You’re so wrong, Jack. You’re so wrong</span><br><span class="line"></span><br><span class="line">The response above is a good example of a rebuttal. It is also a good response for an argumentative essay. The argumentative essay is a genre that is used to present and evaluate an opinion, or to present both sides of a controversial issue, or to present an argument. The purpose of an argumentative essay is not to convince people of a particular viewpoint but to present a particular point of view and to support that point of view with facts.</span><br><span class="line"></span><br><span class="line">The purpose of the rebuttal is not to convince people of an opposing viewpoint but to refute it. In an argumentative essay, the rebuttal is used to present the writer’s opinion and support for that opinion, while the response is used to present the reader’s opinion and to support that opinion.</span><br><span class="line"></span><br><span class="line">In this example, we can see that the writer is not trying to convince Jack that he is wrong, but that he is presenting a particular point of view. The response is not trying to convince Jack that he is wrong, but to refute his argument.</span><br><span class="line"></span><br><span class="line">&lt;h2&gt;&lt;strong&gt;What is the rebuttal of a response?&lt;/strong&gt;&lt;/h2&gt;</span><br><span class="line"></span><br><span class="line">The rebuttal is a response that ref</span><br></pre></td></tr></table></figure></blockquote>
</li>
</ol>
<p>This code demonstrates how we can use a pre-trained model for inference, generating text based on a prompt without any fine-tuning. It sets the stage for further exploration, where fine-tuning on a specific task can improve the model’s performance on related prompts.</p>
<h2 id="Finetuning"><a href="#Finetuning" class="headerlink" title="Finetuning"></a>Finetuning</h2><p>We will perform finetuning using <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2106.09685">Low Rank Adaptation</a> (LoRA).</p>
<p> LoRA is a fine-tuning technique which greatly reduces the number of trainable parameters for downstream tasks by freezing the full weights of the model and inserting a smaller number of new trainable weights into the model. Basically LoRA reparameterizes the larger full weight matrices by 2 smaller low-rank matrices AxB to train and this technique makes training much faster and more memory-efficient.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Enable LoRA for the model and set the LoRA rank to 4.</span></span><br><span class="line">gemma_lm.backbone.enable_lora(rank=<span class="number">4</span>)</span><br><span class="line">gemma_lm.summary()</span><br></pre></td></tr></table></figure>

<p><img src="https://raw.githubusercontent.com/ChaosBeiduo/StoreHouse/main/lora_output_after.png"></p>
<p>Note that enabling LoRA reduces the number of trainable parameters significantly (from 2.6 billion to 2.9 million).</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Limit the input sequence length to 256 (to control memory usage).</span></span><br><span class="line">gemma_lm.preprocessor.sequence_length = <span class="number">256</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Use AdamW (a common optimizer for transformer models).</span></span><br><span class="line">optimizer = keras.optimizers.AdamW(</span><br><span class="line">    learning_rate=<span class="number">5e-5</span>,</span><br><span class="line">    weight_decay=<span class="number">0.01</span>,</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Exclude layernorm and bias terms from decay.</span></span><br><span class="line">optimizer.exclude_from_weight_decay(var_names=[<span class="string">&quot;bias&quot;</span>, <span class="string">&quot;scale&quot;</span>])</span><br><span class="line"></span><br><span class="line">gemma_lm.<span class="built_in">compile</span>(</span><br><span class="line">    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=<span class="literal">True</span>),</span><br><span class="line">    optimizer=optimizer,</span><br><span class="line">    weighted_metrics=[keras.metrics.SparseCategoricalAccuracy()],</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">gemma_lm.fit(filtered_data, epochs=<span class="number">1</span>, batch_size=<span class="number">1</span>)</span><br></pre></td></tr></table></figure>

<h3 id="Inference-after-fine-tuning"><a href="#Inference-after-fine-tuning" class="headerlink" title="Inference after fine-tuning"></a>Inference after fine-tuning</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">prompt = template.<span class="built_in">format</span>(</span><br><span class="line">    prompt=<span class="string">&quot;All work and no play makes Jack a dull boy&quot;</span>,</span><br><span class="line">    response=<span class="string">&quot;&quot;</span>,</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">sampler = keras_nlp.samplers.TopKSampler(k=<span class="number">5</span>, seed=<span class="number">2</span>)</span><br><span class="line">gemma_lm.<span class="built_in">compile</span>(sampler=sampler)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(gemma_lm.generate(prompt, max_length=<span class="number">256</span>))</span><br></pre></td></tr></table></figure>

<blockquote>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">Instruction:</span><br><span class="line">All work and no play makes Jack a dull boy</span><br><span class="line"></span><br><span class="line">Response:</span><br><span class="line">Work is important to be happy, but play is also important to be happy</span><br><span class="line"></span><br><span class="line">Note:</span><br><span class="line">The above statement is from a famous children&#x27;s book, The Boy Who Cried Wolf by Rudyard Kipling, published in 1899.</span><br></pre></td></tr></table></figure>
</blockquote>
<p>It can understand this sentence now!</p>

    </div>

    
    
    

    <footer class="post-footer">

        

          <div class="post-nav">
            <div class="post-nav-item">
                <a href="/2024/12/02/Vue-Star-Rating/" rel="prev" title="[Vue] Star Rating">
                  <i class="fa fa-angle-left"></i> [Vue] Star Rating
                </a>
            </div>
            <div class="post-nav-item">
                <a href="/2024/12/11/Vue-Spinning-Wheel/" rel="next" title="[Vue] Spinning Wheel">
                  [Vue] Spinning Wheel <i class="fa fa-angle-right"></i>
                </a>
            </div>
          </div>
    </footer>
  </article>
</div>






</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">

  <div class="copyright">
    &copy; 
    <span itemprop="copyrightYear">2024</span>
    <span class="with-love">
      <i class="fa-solid fa-dice-d20"></i>
    </span>
    <span class="author" itemprop="copyrightHolder">カオスベイド</span>
  </div>

    </div>
  </footer>

  
  <div class="toggle sidebar-toggle" role="button">
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
  </div>
  <div class="sidebar-dimmer"></div>
  <div class="back-to-top" role="button" aria-label="Back to top">
    <i class="fa fa-arrow-up fa-lg"></i>
    <span>0%</span>
  </div>

  <a href="https://github.com/ChaosBeiduo" class="github-corner" title="Follow me on GitHub" aria-label="Follow me on GitHub" rel="noopener" target="_blank"><svg width="80" height="80" viewBox="0 0 250 250" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg></a>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


  
  <script src="https://cdnjs.cloudflare.com/ajax/libs/animejs/3.2.1/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous"></script>
<script src="/js/comments.js"></script><script src="/js/utils.js"></script><script src="/js/schemes/muse.js"></script><script src="/js/sidebar.js"></script><script src="/js/next-boot.js"></script>

  






  





</body>
</html>
